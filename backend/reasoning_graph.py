from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryBufferMemory  
from langchain_openai import ChatOpenAI
from typing import Dict, List, TypedDict, Any
import json
import hashlib
from datetime import datetime
from langgraph.graph import StateGraph, END
import os

from database_client import db_client


# ==================== StateGraphç‰ˆæœ¬çš„ReasoningGraph ====================

class ReasoningState(TypedDict):
    """ReasoningGraphçš„çŠ¶æ€å®šä¹‰ - åŒ…å«Stage1åˆå¹¶çš„å­—æ®µ"""
    # åŸºç¡€ä¼šè¯çŠ¶æ€
    messages: List[Dict[str, str]]
    user_id: str
    
    # éœ€æ±‚æ”¶é›†çŠ¶æ€
    collected_info: Dict[str, Any]
    stage1_complete: bool
    
    # === ä»Stage1ä¿ç•™çš„çŠ¶æ€å­—æ®µ ===
    extracted_info: Dict[str, Any]  # å½“å‰è½®æå–çš„ä¿¡æ¯
    current_stage: str              # å½“å‰æ‰€åœ¨é˜¶æ®µ
    
    # è¯¦ç»†åº¦è¯„ä¼°çŠ¶æ€
    sufficiency_score: Dict[str, float]  # å„ç»´åº¦sufficiencyè¯„åˆ†
    overall_sufficiency: float           # æ€»ä½“sufficiencyè¯„åˆ†
    sufficiency_threshold: float         # é˜ˆå€¼ (é»˜è®¤75)
    sufficiency_passed: bool
    
    # è¾“å…¥é€‚å®œæ€§æ£€æŸ¥çŠ¶æ€
    input_fitness_result: Dict[str, Any]
    input_fitness_passed: bool
    input_fitness_score: int
    
    # é€‚å®œæ€§æ£€æŸ¥çŠ¶æ€
    fitness_assessment: Dict[str, Any]
    fitness_concerns: List[Dict[str, str]]
    fitness_passed: bool
    
    # æ•…äº‹æ¡†æ¶çŠ¶æ€
    story_framework: str
    story_review_result: Dict[str, Any]
    story_iteration_count: int
    story_framework_approved: bool
    
    # å…³å¡è¯¦ç»†å†…å®¹çŠ¶æ€
    level_details: Dict[str, Any]  # å­˜å‚¨æ¯ä¸ªå…³å¡çš„è§’è‰²å¯¹è¯å’Œåœºæ™¯å‰§æœ¬
    level_generation_status: str   # pending/in_progress/completed/failed
    
    # æœ€ç»ˆçŠ¶æ€
    ready_for_generation: bool
    final_requirements: Dict[str, Any]


class ReasoningGraph:
    """åŸºäºStateGraphçš„æ™ºèƒ½æ¨ç†å›¾ - åˆå¹¶äº†Stage1åŠŸèƒ½"""
    
    def __init__(self, db_client=None):
        self.db_client = db_client or db_client
        
        # åˆå§‹åŒ–LLM
        import os
        from dotenv import load_dotenv
        load_dotenv()
        
        self.llm = ChatOpenAI(
            model="gpt-4o-mini", 
            temperature=0.7,  # å¯¹è¯ç”Ÿæˆä½¿ç”¨è¾ƒé«˜æ¸©åº¦
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # åˆå§‹åŒ–ä¿¡æ¯æå–å™¨
        from info_extractor import create_info_extractor
        self.extractor = create_info_extractor("gpt-4o-mini")
        
        # ===== ä»Stage1ReasoningGraphåˆå¹¶çš„çŠ¶æ€ç®¡ç† =====
        
        # åˆå§‹åŒ–Memory (ä»Stage1åˆå¹¶)
        self.memory = ConversationSummaryBufferMemory(
            max_token_limit=8000,
            llm=self.llm,
            return_messages=True
        )
        
        # æ”¶é›†çš„ä¿¡æ¯å­˜å‚¨ (ä»Stage1åˆå¹¶)
        self.collected_info = {
            "subject": None,
            "grade": None,
            "knowledge_points": None,
            "teaching_goals": None,
            "teaching_difficulties": None,
            "game_style": None,
            "character_design": None,
            "world_setting": None,
            "plot_requirements": None,
            "interaction_requirements": None
        }
        
        # å®Œæˆæ¡ä»¶å®šä¹‰ (ä»Stage1åˆå¹¶)
        self.completion_criteria = {
            "basic_info": ["subject", "grade", "knowledge_points"],
            "teaching_info": ["teaching_goals", "teaching_difficulties"],
            "gamestyle_info": ["game_style", "character_design", "world_setting"],
            "scene_info": ["plot_requirements", "interaction_requirements"]
        }
        
        # å¯¼å…¥promptæ¨¡æ¿ (ä»Stage1åˆå¹¶)
        from prompt_templates import PromptTemplates
        self.prompts = PromptTemplates()
        
        self.graph = self._build_reasoning_graph()
    
    # ===== ä»Stage1ReasoningGraphåˆå¹¶çš„æ–¹æ³• =====
    
    def determine_current_stage(self) -> str:
        """ç¡®å®šå½“å‰åº”è¯¥æ”¶é›†å“ªä¸ªé˜¶æ®µçš„ä¿¡æ¯"""
        if not self._stage_completed("basic_info"):
            return "basic_info"
        elif not self._stage_completed("teaching_info"):
            return "teaching_info"
        elif not self._stage_completed("gamestyle_info"):
            return "gamestyle_info"
        elif not self._stage_completed("scene_info"):
            return "scene_info"
        else:
            return "complete"

    def _stage_completed(self, stage: str) -> bool:
        """æ£€æŸ¥ç‰¹å®šé˜¶æ®µæ˜¯å¦å®Œæˆ"""
        required_fields = self.completion_criteria[stage]
        for field in required_fields:
            value = self.collected_info.get(field)
            if not value:
                return False
            if isinstance(value, list) and len(value) == 0:
                return False
        return True
        
    async def extract_info(self, user_input: str, stage: str = "basic_info") -> Dict:
        """æå–ç”¨æˆ·è¾“å…¥ä¸­çš„ä¿¡æ¯"""
        return await self.extractor.extract_from_user_input(user_input, stage)

    def update_state(self, extracted_info: Dict) -> None:
        """æ›´æ–°æ”¶é›†çŠ¶æ€"""
        print(f"DEBUG update_state: received = {extracted_info}")
        for key, value in extracted_info.items():
            print(f"DEBUG: processing key={key}, value={value}")
            if value and key in self.collected_info:
                if isinstance(value, list):
                    # å¤„ç†åˆ—è¡¨ç±»å‹çš„æ•°æ®
                    if self.collected_info[key]:
                        # åˆå¹¶åˆ—è¡¨ï¼Œå»é‡
                        existing = self.collected_info[key] if isinstance(self.collected_info[key], list) else [self.collected_info[key]]
                        combined = existing + value
                        self.collected_info[key] = list(set(combined))
                    else:
                        self.collected_info[key] = value
                else:
                    # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„æ•°æ®
                    self.collected_info[key] = value
        
        print(f"DEBUG update_state: final collected_info = {self.collected_info}")

    def check_stage_completion(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¾¾æˆStage1ç›®æ ‡"""
        return self.determine_current_stage() == "complete"
    
    def get_lacked_info(self) -> Dict:
        """è·å–ç¼ºå¤±ä¿¡æ¯è¯¦æƒ…"""
        current_stage = self.determine_current_stage()

        if current_stage == "complete":
            return {"stage": "complete", "missing_fields": [], "missing_details": {}, "completion_rate": 1.0}
        
        print(f"current stage is {current_stage}")
        # æ ¹æ®å½“å‰é˜¶æ®µè·å–ç¼ºå¤±ä¿¡æ¯
        if current_stage == "basic_info":
            return self._check_basic_info_gaps()
        elif current_stage == "teaching_info":
            return self._check_teaching_info_gaps()
        elif current_stage == "gamestyle_info":
            return self._check_gamestyle_info_gaps()
        elif current_stage == "scene_info":
            return self._check_scene_info_gaps()
        else:
            return {"stage": current_stage, "missing_fields": [], "missing_details": {}, "completion_rate": 0.0}

    def _check_basic_info_gaps(self) -> Dict:
        """æ£€æŸ¥åŸºç¡€ä¿¡æ¯ç¼ºå¤±"""
        missing_fields = []
        missing_details = {}
        
        if not self.collected_info.get("subject"):
            missing_fields.append("subject")
            missing_details["subject"] = "å­¦ç§‘é¢†åŸŸ"
        if not self.collected_info.get("grade"):
            missing_fields.append("grade")
            missing_details["grade"] = "ç›®æ ‡å¹´çº§"
        if not self.collected_info.get("knowledge_points"):
            missing_fields.append("knowledge_points")
            missing_details["knowledge_points"] = "å…·ä½“çŸ¥è¯†ç‚¹"
        
        completion_rate = 1.0 - len(missing_fields) / 3
        return {
            "stage": "basic_info",
            "missing_fields": missing_fields,
            "missing_details": missing_details,
            "completion_rate": completion_rate
        }

    def _check_teaching_info_gaps(self) -> Dict:
        """æ£€æŸ¥æ•™å­¦ä¿¡æ¯ç¼ºå¤±"""
        missing_fields = []
        missing_details = {}
        
        if not self.collected_info.get("teaching_goals"):
            missing_fields.append("teaching_goals")
            missing_details["teaching_goals"] = "æ•™å­¦ç›®æ ‡"
        if not self.collected_info.get("teaching_difficulties"):
            missing_fields.append("teaching_difficulties")
            missing_details["teaching_difficulties"] = "æ•™å­¦éš¾ç‚¹"
        
        completion_rate = 1.0 - len(missing_fields) / 2
        return {
            "stage": "teaching_info",
            "missing_fields": missing_fields,
            "missing_details": missing_details,
            "completion_rate": completion_rate
        }

    def _check_gamestyle_info_gaps(self) -> Dict:
        """æ£€æŸ¥æ¸¸æˆé£æ ¼ä¿¡æ¯ç¼ºå¤±"""
        missing_fields = []
        missing_details = {}
        
        if not self.collected_info.get("game_style"):
            missing_fields.append("game_style")
            missing_details["game_style"] = "æ¸¸æˆé£æ ¼"
        if not self.collected_info.get("character_design"):
            missing_fields.append("character_design")
            missing_details["character_design"] = "è§’è‰²è®¾è®¡"
        if not self.collected_info.get("world_setting"):
            missing_fields.append("world_setting")
            missing_details["world_setting"] = "ä¸–ç•ŒèƒŒæ™¯"
        
        completion_rate = 1.0 - len(missing_fields) / 3
        return {
            "stage": "gamestyle_info",
            "missing_fields": missing_fields,
            "missing_details": missing_details,
            "completion_rate": completion_rate
        }

    def _check_scene_info_gaps(self) -> Dict:
        """æ£€æŸ¥æƒ…èŠ‚ä¿¡æ¯ç¼ºå¤±"""
        missing_fields = []
        missing_details = {}
        
        if not self.collected_info.get("plot_requirements"):
            missing_fields.append("plot_requirements")
            missing_details["plot_requirements"] = "æƒ…èŠ‚éœ€æ±‚"
        if not self.collected_info.get("interaction_requirements"):
            missing_fields.append("interaction_requirements")
            missing_details["interaction_requirements"] = "äº’åŠ¨æ–¹å¼"
        
        completion_rate = 1.0 - len(missing_fields) / 2
        return {
            "stage": "scene_info",
            "missing_fields": missing_fields,
            "missing_details": missing_details,
            "completion_rate": completion_rate
        }
    
    async def generate_response_with_lacked_info(self, lacked_info: Dict) -> str:
        """åŸºäºç¼ºå¤±ä¿¡æ¯ç”Ÿæˆå›å¤"""
        # è·å–åŠ¨æ€prompt
        dynamic_prompt = self.prompts.generate_dynamic_prompt(
            lacked_info["stage"],
            self.collected_info,
            lacked_info
        )
        print(f"dynamic prompt is : {dynamic_prompt}")
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=dynamic_prompt
        )
        
        # ç”Ÿæˆå›å¤ï¼ˆç©ºè¾“å…¥ï¼Œè®©æ¨¡æ¿å¤„ç†ï¼‰
        response = await conversation.apredict(input="")
        return response.strip()
    
    def save_final_requirements(self) -> Dict:
        """ä¿å­˜æœ€ç»ˆæ”¶é›†çš„éœ€æ±‚ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            if not self.db_client:
                return {
                    "success": False,
                    "message": "æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•ä¿å­˜",
                    "timestamp": datetime.now().isoformat()
                }
            
            # ç”Ÿæˆå”¯ä¸€ID
            timestamp = datetime.now().isoformat()
            content_hash = hashlib.md5(json.dumps(self.collected_info, sort_keys=True).encode()).hexdigest()[:8]
            requirement_id = f"requirement_{timestamp}_{content_hash}"
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            requirement_data = {
                "id": requirement_id,
                "user_id": "default_user",  # ç®€åŒ–ä¸ºdefault
                "timestamp": timestamp,
                "collected_info": self.collected_info,
                "final_requirements": self.collected_info,  # ä¸ºäº†å…¼å®¹æ€§
                "status": "completed"
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            save_success = self.db_client.save_requirement(requirement_data)
            
            if save_success:
                return {
                    "success": True,
                    "requirement_id": requirement_id,
                    "message": "éœ€æ±‚ä¿¡æ¯å·²æˆåŠŸä¿å­˜",
                    "timestamp": timestamp
                }
            else:
                return {
                    "success": False,
                    "message": "ä¿å­˜å¤±è´¥",
                    "timestamp": timestamp
                }
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜éœ€æ±‚ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"ä¿å­˜å¤±è´¥: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    def _build_reasoning_graph(self) -> StateGraph:
        """æ„å»ºæ¨ç†çŠ¶æ€å›¾"""
        
        workflow = StateGraph(ReasoningState)
        
        # ==================== èŠ‚ç‚¹å®šä¹‰ ====================
        
        # æ–°æµç¨‹ï¼šStage1é›†æˆçš„èŠ‚ç‚¹
        workflow.add_node("check_info_completed", self._check_info_completed)
        workflow.add_node("check_input_fitness", self._check_input_fitness)
        workflow.add_node("extract_and_update_info", self._extract_and_update_info)
        workflow.add_node("determine_stage", self._determine_stage)
        workflow.add_node("generate_lack_response", self._generate_lack_response)
        
        # ä¿ç•™ï¼šåç»­è¯¦ç»†åº¦å’Œé€‚å®œæ€§æ£€æŸ¥èŠ‚ç‚¹
        workflow.add_node("need_more_details", self._assess_sufficiency)
        workflow.add_node("generate_need_more_details_response", self._generate_sufficiency_questions)
        workflow.add_node("check_fitness", self._check_fitness)
        workflow.add_node("generate_negotiate_response", self._generate_negotiate_response)
        workflow.add_node("generate_finish_response", self._generate_finish_response)
        
        # æ•…äº‹æ¡†æ¶ç”ŸæˆèŠ‚ç‚¹
        workflow.add_node("generate_story_framework", self._generate_story_framework)
        workflow.add_node("review_story_framework", self._review_story_framework)
        workflow.add_node("improve_story_framework", self._improve_story_framework)
        workflow.add_node("distribute_to_levels", self._distribute_to_levels)
        
        # å…³å¡å†…ä¸²è¡Œã€å…³å¡é—´å¹¶è¡Œçš„ç”ŸæˆèŠ‚ç‚¹ - ä½¿ç”¨å¾ªç¯å’Œpartial  
        from functools import partial
        for level in range(1, 7):
            # åœºæ™¯è§†è§‰å’Œå‰§æœ¬ç”ŸæˆèŠ‚ç‚¹ï¼ˆç¬¬ä¸€æ­¥ï¼‰
            workflow.add_node(f"generate_level_{level}_scenes", 
                             partial(self._generate_level_scenes, level=level))
            # è§’è‰²å¯¹è¯å’Œè§’è‰²ä»‹ç»ç”ŸæˆèŠ‚ç‚¹ï¼ˆç¬¬äºŒæ­¥ï¼Œä½¿ç”¨åœºæ™¯æ•°æ®ï¼‰
            workflow.add_node(f"generate_level_{level}_characters", 
                             partial(self._generate_level_characters, level=level))
        
        # æœ€ç»ˆæ±‡èšèŠ‚ç‚¹ï¼šç­‰å¾…æ‰€æœ‰å¯¹è¯å®Œæˆ  
        workflow.add_node("collect_all_levels", self._collect_all_level_results)
        
        # ==================== æµç¨‹è·¯ç”± ====================
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("check_info_completed")
        
        # æ–°æµç¨‹ï¼šå…ˆæ£€æŸ¥è¾“å…¥é€‚å®œæ€§
        workflow.add_edge("check_info_completed", "check_input_fitness")
        
        # è¾“å…¥é€‚å®œæ€§æ£€æŸ¥åçš„æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "check_input_fitness",
            self._should_proceed_with_input,
            {
                "proceed": "extract_and_update_info",
                "reject": END  # ç›´æ¥ç»“æŸï¼Œåœ¨_check_input_fitnessä¸­ä¼šæ·»åŠ æ‹’ç»æ¶ˆæ¯
            }
        )
        
        # ä¿¡æ¯æå–åæµè½¬åˆ°é˜¶æ®µåˆ¤æ–­
        workflow.add_edge("extract_and_update_info", "determine_stage")
        
        # é˜¶æ®µåˆ¤æ–­åçš„æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "determine_stage",
            self._decide_stage_routing,
            {
                "info_incomplete": "generate_lack_response",
                "info_complete": "need_more_details"  # ç»§ç»­åŸæœ‰æµç¨‹
            }
        )
        
        # ä¿¡æ¯ä¸è¶³æ—¶ç»“æŸï¼ˆç­‰å¾…ç”¨æˆ·è¡¥å……ï¼‰
        workflow.add_edge("generate_lack_response", END)
        
        # é˜¶æ®µ2è·¯ç”±ï¼šæ£€æŸ¥è¯¦ç»†åº¦å……è¶³æ€§
        workflow.add_conditional_edges(
            "need_more_details",
            self._decide_after_sufficiency_check,
            {
                "need_more_details": "generate_need_more_details_response",
                "sufficiency_passed": "check_fitness"
            }
        )
        
        # éœ€è¦æ›´å¤šç»†èŠ‚æ—¶ï¼Œç”Ÿæˆé—®é¢˜åç»“æŸï¼ˆç­‰å¾…ç”¨æˆ·å›ç­”ï¼‰
        workflow.add_edge("generate_need_more_details_response", END)
        
        # é˜¶æ®µ3è·¯ç”±ï¼šæ£€æŸ¥é€‚å®œæ€§
        workflow.add_conditional_edges(
            "check_fitness",
            self._decide_after_fitness_check,
            {
                "fitness_concerns": "generate_negotiate_response",
                "fitness_passed": "generate_finish_response"
            }
        )
        
        # æœ‰é€‚å®œæ€§é—®é¢˜æ—¶ï¼Œç”Ÿæˆåå•†å›å¤åç»“æŸï¼ˆç­‰å¾…ç”¨æˆ·å›åº”ï¼‰
        workflow.add_edge("generate_negotiate_response", END)
        
        # æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œç”Ÿæˆå®Œæˆå›å¤åè¿›å…¥æ•…äº‹æ¡†æ¶ç”Ÿæˆ
        workflow.add_edge("generate_finish_response", "generate_story_framework")
        
        # æ•…äº‹æ¡†æ¶ç”Ÿæˆåè¿›è¡Œå®¡æ ¸
        workflow.add_edge("generate_story_framework", "review_story_framework")
        
        # æ•…äº‹æ¡†æ¶å®¡æ ¸åçš„æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "review_story_framework",
            self._should_continue_story_iteration,
            {
                "max_reached": END,  # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
                "continue_iteration": "improve_story_framework",  # éœ€è¦æ”¹è¿›
                "approved": "distribute_to_levels"  # å®¡æ ¸é€šè¿‡ï¼Œåˆ†å‘åˆ°æ‰€æœ‰å…³å¡
            }
        )
        
        # å®¡æ ¸é€šè¿‡åï¼šå…³å¡å†…ä¸²è¡Œï¼ˆscene -> dialogueï¼‰ï¼Œå…³å¡é—´å¹¶è¡Œ
        for level in range(1, 7):
            # ä»distribute_to_levelså¹¶è¡Œåˆ†å‘åˆ°æ¯ä¸ªå…³å¡çš„åœºæ™¯ç”Ÿæˆ
            workflow.add_edge("distribute_to_levels", f"generate_level_{level}_scenes")
            # æ¯ä¸ªå…³å¡çš„åœºæ™¯å®Œæˆåç”Ÿæˆå¯¹è¯
            workflow.add_edge(f"generate_level_{level}_scenes", f"generate_level_{level}_characters")
            # æ¯ä¸ªå…³å¡çš„å¯¹è¯å®Œæˆåæ±‡èšåˆ°collect_all_levels
            workflow.add_edge(f"generate_level_{level}_characters", "collect_all_levels")
        
        # æ±‡èšå®Œæˆåç»“æŸ
        workflow.add_edge("collect_all_levels", END)
        
        # æ”¹è¿›æ•…äº‹æ¡†æ¶åé‡æ–°å®¡æ ¸
        workflow.add_edge("improve_story_framework", "review_story_framework")
        
        # ç¼–è¯‘å›¾
        return workflow.compile()
    
    # ==================== å†³ç­–é€»è¾‘ ====================
    
    def _decide_after_sufficiency_check(self, state: ReasoningState) -> str:
        """sufficiencyæ£€æŸ¥åçš„è·¯ç”±å†³ç­–"""
        if state.get("sufficiency_passed", False):
            print("âœ… è¯¦ç»†åº¦æ£€æŸ¥é€šè¿‡ï¼Œè¿›å…¥é€‚å®œæ€§æ£€æŸ¥")
            return "sufficiency_passed"
        else:
            print("âŒ è¯¦ç»†åº¦ä¸è¶³ï¼Œéœ€è¦æ›´å¤šä¿¡æ¯")
            return "need_more_details"
    
    def _decide_after_fitness_check(self, state: ReasoningState) -> str:
        """fitnessæ£€æŸ¥åçš„è·¯ç”±å†³ç­–"""
        fitness_concerns = state.get("fitness_concerns", [])
        
        if fitness_concerns:
            print(f"âš ï¸ å‘ç°é€‚å®œæ€§é—®é¢˜: {len(fitness_concerns)}ä¸ª")
            return "fitness_concerns"
        else:
            print("âœ… é€‚å®œæ€§æ£€æŸ¥é€šè¿‡ï¼Œå‡†å¤‡å®Œæˆ")
            return "fitness_passed"
    
    def _should_proceed_with_input(self, state: ReasoningState) -> str:
        """åˆ¤æ–­è¾“å…¥æ˜¯å¦å¯ä»¥ç»§ç»­å¤„ç†"""
        if state["input_fitness_passed"]:
            return "proceed"
        else:
            return "reject"
    
    def _should_continue_story_iteration(self, state: ReasoningState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æ•…äº‹æ¡†æ¶è¿­ä»£"""
        max_iterations = 3  # æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        if state["story_framework_approved"]:
            return "approved"
        elif state["story_iteration_count"] >= max_iterations:
            print(f"âš ï¸ å·²è¾¾æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶é€šè¿‡")
            return "max_reached"
        else:
            return "continue_iteration"
    
    # ==================== æ–°å¢èŠ‚ç‚¹å‡½æ•° ====================
    
    async def _check_input_fitness(self, state: ReasoningState) -> ReasoningState:
        """æ£€æŸ¥ç”¨æˆ·è¾“å…¥çš„é€‚å®œæ€§"""
        print("ğŸ›¡ï¸ æ£€æŸ¥è¾“å…¥é€‚å®œæ€§...")
        
        # è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥
        user_messages = [msg for msg in state["messages"] if msg["role"] == "user"]
        if not user_messages:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·è¾“å…¥")
            state["input_fitness_passed"] = False
            return state
            
        latest_user_input = user_messages[-1]["content"]
        
        # ä½¿ç”¨LLMè¿›è¡Œè¾“å…¥é€‚å®œæ€§æ£€æŸ¥
        fitness_result = await self._llm_check_input_fitness(latest_user_input, state["collected_info"])
        
        # æ›´æ–°çŠ¶æ€
        state["input_fitness_result"] = fitness_result
        state["input_fitness_passed"] = fitness_result.get("input_fitness") == "passed"
        state["input_fitness_score"] = fitness_result.get("fitness_score", 0)
        
        if state["input_fitness_passed"]:
            print("âœ… è¾“å…¥é€‚å®œæ€§æ£€æŸ¥é€šè¿‡")
        else:
            issues_count = len(fitness_result.get("issues", []))
            print(f"âŒ è¾“å…¥é€‚å®œæ€§æ£€æŸ¥æœªé€šè¿‡ï¼Œå‘ç°{issues_count}ä¸ªé—®é¢˜")
            for issue in fitness_result.get("issues", []):
                print(f"  - {issue.get('category', 'æœªçŸ¥')}: {issue.get('description', 'æ— æè¿°')}")
            
            # ç”Ÿæˆæ‹’ç»å›å¤
            rejection_message = self._generate_input_rejection_message(fitness_result)
            state["messages"].append({
                "role": "assistant",
                "content": rejection_message
            })
        
        return state

    def _generate_input_rejection_message(self, fitness_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¾“å…¥æ‹’ç»æ¶ˆæ¯"""
        issues = fitness_result.get("issues", [])
        
        message_parts = ["ğŸ˜” æŠ±æ­‰ï¼Œæ‚¨æåˆ°çš„å†…å®¹å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæˆ‘æ— æ³•ç»§ç»­å¤„ç†ï¼š"]
        
        for issue in issues:
            category = issue.get("category", "æœªçŸ¥é—®é¢˜")
            description = issue.get("description", "")
            suggestion = issue.get("suggestion", "")
            
            message_parts.append(f"\nâ€¢ **{category}**: {description}")
            if suggestion:
                message_parts.append(f"  å»ºè®®: {suggestion}")
        
        message_parts.append("\n\nğŸ¯ è¯·æä¾›ç¬¦åˆæ•™è‚²è§„èŒƒã€é€»è¾‘åˆç†ä¸”é€‚åˆå­¦ç”Ÿå¹´é¾„çš„æ¸¸æˆè®¾è®¡éœ€æ±‚ã€‚æˆ‘å°†å¾ˆé«˜å…´ä¸ºæ‚¨è®¾è®¡ä¸€ä¸ªä¼˜ç§€çš„æ•™è‚²æ¸¸æˆï¼")
        
        return "".join(message_parts)

    async def _generate_requirement_analysis_report(self, collected_info: Dict[str, Any], 
                                                  sufficiency_scores: Dict[str, float]) -> str:
        """ä½¿ç”¨LLMç”ŸæˆRPGéœ€æ±‚åˆ†ææŠ¥å‘Š"""
        
        # è®¡ç®—å¹³å‡åˆ†
        average_score = sum(sufficiency_scores.values()) / len(sufficiency_scores) if sufficiency_scores else 75
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_requirement_analysis_prompt()
        analysis_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            sufficiency_scores=sufficiency_scores,
            average_score=average_score
        )

        try:
            report = await self.llm.apredict(analysis_prompt)
            return report.strip()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆéœ€æ±‚åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æŠ¥å‘Š
            return f"""RPGæ•™è‚²æ¸¸æˆéœ€æ±‚åˆ†ææŠ¥å‘Š

ã€é¡¹ç›®åŸºç¡€ä¿¡æ¯ã€‘
å­¦ç§‘: {collected_info.get('subject', 'æœªæŒ‡å®š')}
å¹´çº§: {collected_info.get('grade', 'æœªæŒ‡å®š')} 
çŸ¥è¯†ç‚¹: {collected_info.get('knowledge_points', 'æœªæŒ‡å®š')}

ã€æ¸¸æˆè®¾è®¡è¦ç´ ã€‘
æ¸¸æˆé£æ ¼: {collected_info.get('game_style', 'æœªæŒ‡å®š')}
è§’è‰²è®¾è®¡: {collected_info.get('character_design', 'æœªæŒ‡å®š')}
ä¸–ç•ŒèƒŒæ™¯: {collected_info.get('world_setting', 'æœªæŒ‡å®š')}

æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"""

    async def _llm_generate_story_framework(self, collected_info: Dict[str, Any], 
                                          sufficiency_scores: Dict[str, float],
                                          analysis_report: str = "") -> str:
        """ä½¿ç”¨LLMç”ŸæˆRPGæ•…äº‹æ¡†æ¶"""
        
        # è®¡ç®—å¹³å‡åˆ†
        average_score = sum(sufficiency_scores.values()) / len(sufficiency_scores) if sufficiency_scores else 75
        
        # æå–å…³é”®ä¿¡æ¯ç”¨äºæ¨¡æ¿
        knowledge_points = collected_info.get('knowledge_points', [])
        knowledge_points_str = ', '.join(knowledge_points) if isinstance(knowledge_points, list) else str(knowledge_points)
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_story_framework_generation_prompt()
        framework_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            sufficiency_scores=sufficiency_scores,
            average_score=average_score,
            knowledge_points=knowledge_points_str,
            grade=collected_info.get('grade', 'æœªæŒ‡å®š'),
            game_style=collected_info.get('game_style', 'æœªæŒ‡å®š'),
            world_setting=collected_info.get('world_setting', 'æœªæŒ‡å®š'),
            character_design=collected_info.get('character_design', 'æœªæŒ‡å®š')
        )

        try:
            framework = await self.llm.apredict(framework_prompt)
            return framework.strip()
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ•…äº‹æ¡†æ¶å¤±è´¥: {e}")
            return f"""RPGæ•…äº‹æ¡†æ¶ç”Ÿæˆå¤±è´¥
            
ã€åŸºç¡€ä¿¡æ¯ã€‘
å­¦ç§‘: {collected_info.get('subject', 'æœªæŒ‡å®š')}
å¹´çº§: {collected_info.get('grade', 'æœªæŒ‡å®š')}
çŸ¥è¯†ç‚¹: {knowledge_points_str}

ã€é”™è¯¯ä¿¡æ¯ã€‘
æ•…äº‹æ¡†æ¶ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚
é”™è¯¯è¯¦æƒ…: {str(e)}"""

    async def _llm_review_story_framework(self, collected_info: Dict[str, Any], 
                                        story_framework: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMå®¡æ ¸æ•…äº‹æ¡†æ¶"""
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_story_review_prompt()
        review_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            story_framework=story_framework
        )

        try:
            response = await self.llm.apredict(review_prompt)
            json_content = self._extract_json_from_markdown(response.strip())
            result = json.loads(json_content)
            return result
        except Exception as e:
            print(f"âŒ æ•…äº‹æ¡†æ¶å®¡æ ¸å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ä¸é€šè¿‡çš„ç»“æœ
            return {
                "ä¸»çº¿æ˜ç¡®æ€§": {
                    "åˆ†æ•°": 60,
                    "è¯„ä»·": "å®¡æ ¸è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                    "æ”¹è¿›å»ºè®®": "è¯·é‡æ–°ç”Ÿæˆæ•…äº‹æ¡†æ¶"
                },
                "å†…å®¹ä¸€è‡´æ€§": {"åˆ†æ•°": 60, "è¯„ä»·": "å®¡æ ¸é”™è¯¯", "æ”¹è¿›å»ºè®®": "é‡æ–°ç”Ÿæˆ"},
                "å‰§æƒ…è¿è´¯æ€§": {"åˆ†æ•°": 60, "è¯„ä»·": "å®¡æ ¸é”™è¯¯", "æ”¹è¿›å»ºè®®": "é‡æ–°ç”Ÿæˆ"},
                "æ•™è‚²èåˆåº¦": {"åˆ†æ•°": 60, "è¯„ä»·": "å®¡æ ¸é”™è¯¯", "æ”¹è¿›å»ºè®®": "é‡æ–°ç”Ÿæˆ"},
                "å¸å¼•åŠ›è¯„ä¼°": {"åˆ†æ•°": 60, "è¯„ä»·": "å®¡æ ¸é”™è¯¯", "æ”¹è¿›å»ºè®®": "é‡æ–°ç”Ÿæˆ"},
                "æ€»åˆ†": 60.0,
                "æ•´ä½“è¯„ä»·": f"å®¡æ ¸è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                "æ˜¯å¦é€šè¿‡": False,
                "é‡ç‚¹æ”¹è¿›æ–¹å‘": ["ä¿®å¤ç³»ç»Ÿé”™è¯¯", "é‡æ–°ç”Ÿæˆæ¡†æ¶"]
            }

    async def _llm_improve_story_framework(self, collected_info: Dict[str, Any],
                                         current_framework: str,
                                         review_feedback: Dict[str, Any]) -> str:
        """ä½¿ç”¨LLMæ”¹è¿›æ•…äº‹æ¡†æ¶"""
        
        # æ„å»ºæ”¹è¿›æŒ‡å¯¼
        improvement_focus = review_feedback.get("é‡ç‚¹æ”¹è¿›æ–¹å‘", [])
        improvement_focus_str = ', '.join(improvement_focus)
        
        specific_improvements = []
        for dimension, details in review_feedback.items():
            if isinstance(details, dict) and details.get("åˆ†æ•°", 100) < 75:
                specific_improvements.append(f"- {dimension}: {details.get('æ”¹è¿›å»ºè®®', 'éœ€è¦æ”¹è¿›')}")
        
        specific_improvements_str = '\n'.join(specific_improvements) if specific_improvements else "æ•´ä½“ä¼˜åŒ–æ•…äº‹è®¾è®¡"
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_story_improvement_prompt()
        improvement_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            current_framework=current_framework,
            review_feedback=str(review_feedback),
            improvement_focus=improvement_focus_str,
            specific_improvements=specific_improvements_str
        )

        try:
            improved_framework = await self.llm.apredict(improvement_prompt)
            return improved_framework.strip()
        except Exception as e:
            print(f"âŒ æ”¹è¿›æ•…äº‹æ¡†æ¶å¤±è´¥: {e}")
            return current_framework  # è¿”å›åŸæ¡†æ¶

    async def _extract_and_update_info(self, state: ReasoningState) -> ReasoningState:
        """æå–å¹¶æ›´æ–°ç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯"""
        try:
            # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
            messages = state.get("messages", [])
            if not messages:
                return state
                
            latest_message = messages[-1]
            if latest_message.get("role") != "user":
                return state
                
            user_input = latest_message.get("content", "")
            current_stage = state.get("current_stage", "basic_info")
            
            # æå–ä¿¡æ¯
            extracted_info = await self.extract_info(user_input, current_stage)
            print(f"DEBUG: æå–åˆ°çš„ä¿¡æ¯: {extracted_info}")
            
            # æ›´æ–°çŠ¶æ€
            self.update_state(extracted_info)
            
            # æ›´æ–°stateä¸­çš„ä¿¡æ¯
            state["extracted_info"] = extracted_info
            state["collected_info"] = self.collected_info.copy()
            
            return state
            
        except Exception as e:
            print(f"âŒ ä¿¡æ¯æå–å¤±è´¥: {e}")
            return state
    
    async def _determine_stage(self, state: ReasoningState) -> ReasoningState:
        """ç¡®å®šå½“å‰é˜¶æ®µå’Œå®ŒæˆçŠ¶æ€"""
        try:
            # ç¡®å®šå½“å‰é˜¶æ®µ
            current_stage = self.determine_current_stage()
            stage1_complete = self.check_stage_completion()
            
            print(f"DEBUG: å½“å‰é˜¶æ®µ: {current_stage}, Stage1å®Œæˆ: {stage1_complete}")
            
            # æ›´æ–°çŠ¶æ€
            state["current_stage"] = current_stage
            state["stage1_complete"] = stage1_complete
            
            # å¦‚æœå®Œæˆï¼Œè®¾ç½®ready_for_generationå’Œä¿å­˜éœ€æ±‚
            if stage1_complete:
                save_result = self.save_final_requirements()
                if save_result["success"]:
                    state["ready_for_generation"] = True
                    state["final_requirements"] = self.collected_info.copy()
                    print(f"âœ… éœ€æ±‚ä¿¡æ¯å·²ä¿å­˜: {save_result['requirement_id']}")
            
            return state
            
        except Exception as e:
            print(f"âŒ é˜¶æ®µåˆ¤æ–­å¤±è´¥: {e}")
            return state
    
    def _decide_stage_routing(self, state: ReasoningState) -> str:
        """å†³å®šé˜¶æ®µè·¯ç”±ï¼šå®Œæˆæˆ–æœªå®Œæˆ"""
        stage1_complete = state.get("stage1_complete", False)
        
        if stage1_complete:
            return "info_complete"
        else:
            return "info_incomplete"
    
    # ==================== èŠ‚ç‚¹å®ç° ====================
    
    async def _check_info_completed(self, state: ReasoningState) -> ReasoningState:
        """æ£€æŸ¥åŸºç¡€ä¿¡æ¯å®Œæ•´æ€§ - ç®€åŒ–ç‰ˆï¼Œä¸»è¦ç”¨äºåˆå§‹åŒ–"""
        print("ğŸ” æ£€æŸ¥åŸºç¡€ä¿¡æ¯å®Œæ•´æ€§...")
        
        # åŒæ­¥çŠ¶æ€
        self.collected_info = state["collected_info"]
        
        # è¿™ä¸ªèŠ‚ç‚¹ç°åœ¨ä¸»è¦æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œå®é™…æ£€æŸ¥åœ¨determine_stageä¸­è¿›è¡Œ
        print(f"å½“å‰collected_info: {self.collected_info}")
            
        return state
    
    async def _generate_lack_response(self, state: ReasoningState) -> ReasoningState:
        """ç”Ÿæˆä¿¡æ¯ä¸è¶³çš„å›å¤ - ä½¿ç”¨åˆå¹¶çš„Stage1é€»è¾‘"""
        print("ğŸ“ ç”Ÿæˆä¿¡æ¯ä¸è¶³å›å¤...")
        print(f"current collected_info is {self.collected_info}")
        
        # åŒæ­¥çŠ¶æ€
        self.collected_info = state["collected_info"]
        
        # è·å–ç¼ºå¤±ä¿¡æ¯è¯¦æƒ…
        lacked_info = self.get_lacked_info()
        
        # ç”Ÿæˆå›å¤
        response = await self.generate_response_with_lacked_info(lacked_info)
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append({
            "role": "assistant", 
            "content": response,
            "stage": lacked_info["stage"],
            "lacked_info": lacked_info,
            "timestamp": datetime.now().isoformat()
        })
        
        return state
    
    async def _assess_sufficiency(self, state: ReasoningState) -> ReasoningState:
        """è¯„ä¼°ä¿¡æ¯è¯¦ç»†åº¦å……è¶³æ€§"""
        print("ğŸ” è¯„ä¼°ä¿¡æ¯è¯¦ç»†åº¦...")
        
        collected_info = state["collected_info"]
        conversation_context = self._build_conversation_context(state["messages"])
        
        # ä½¿ç”¨LLMè¯„ä¼°å„ä¸ªç»´åº¦çš„è¯¦ç»†åº¦
        sufficiency_assessment = await self._llm_assess_sufficiency(collected_info, conversation_context)
        
        # æ›´æ–°çŠ¶æ€
        state["sufficiency_score"] = sufficiency_assessment["dimension_scores"]
        state["overall_sufficiency"] = sufficiency_assessment["overall_score"]
        state["sufficiency_passed"] = sufficiency_assessment["overall_score"] >= state["sufficiency_threshold"]
        
        print(f"ğŸ“Š è¯¦ç»†åº¦è¯„ä¼°å®Œæˆ:")
        for dimension, score in sufficiency_assessment["dimension_scores"].items():
            print(f"  {dimension}: {score:.1f}/100")
        print(f"  æ€»ä½“è¯„åˆ†: {sufficiency_assessment['overall_score']:.1f}/100 (é˜ˆå€¼: {state['sufficiency_threshold']})")
        
        return state
        
    async def _generate_sufficiency_questions(self, state: ReasoningState) -> ReasoningState:
        """ç”Ÿæˆè¯¦ç»†åº¦è¡¥å……é—®é¢˜"""
        print("â“ ç”Ÿæˆè¯¦ç»†åº¦è¡¥å……é—®é¢˜...")
        
        # è·å–è¯„ä¼°ç»“æœ
        sufficiency_scores = state["sufficiency_score"]
        overall_score = state["overall_sufficiency"]
        
        # ç”Ÿæˆé’ˆå¯¹æ€§çš„è¡¥å……é—®é¢˜
        questions_response = await self._llm_generate_sufficiency_questions(
            collected_info=state["collected_info"],
            sufficiency_scores=sufficiency_scores,
            overall_score=overall_score,
            conversation_context=self._build_conversation_context(state["messages"])
        )
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append({
            "role": "assistant",
            "content": questions_response,
            "type": "sufficiency_questions",
            "sufficiency_scores": sufficiency_scores
        })
        
        return state
        
    async def _check_fitness(self, state: ReasoningState) -> ReasoningState:
        """æ£€æŸ¥å†…å®¹é€‚å®œæ€§"""
        print("ğŸ›¡ï¸ æ£€æŸ¥å†…å®¹é€‚å®œæ€§...")
        
        # è·å–æ”¶é›†çš„ä¿¡æ¯
        collected_info = state["collected_info"]
        conversation_context = self._build_conversation_context(state["messages"])
        
        # ä½¿ç”¨LLMè¿›è¡Œé€‚å®œæ€§æ£€æŸ¥
        fitness_result = await self._llm_check_fitness(collected_info, conversation_context)
        
        # æ›´æ–°çŠ¶æ€
        state["fitness_assessment"] = fitness_result
        state["fitness_concerns"] = fitness_result.get("concerns", [])
        state["fitness_passed"] = len(fitness_result.get("concerns", [])) == 0
        
        if state["fitness_passed"]:
            print("âœ… é€‚å®œæ€§æ£€æŸ¥é€šè¿‡")
        else:
            concern_count = len(state["fitness_concerns"])
            print(f"âš ï¸ å‘ç°{concern_count}ä¸ªé€‚å®œæ€§é—®é¢˜éœ€è¦å¤„ç†")
        
        return state
        
    async def _generate_negotiate_response(self, state: ReasoningState) -> ReasoningState:
        """ç”Ÿæˆé€‚å®œæ€§åå•†å›å¤"""
        print("ğŸ¤ ç”Ÿæˆé€‚å®œæ€§åå•†å›å¤...")
        
        # è·å–é€‚å®œæ€§æ£€æŸ¥ç»“æœ
        fitness_assessment = state["fitness_assessment"]
        fitness_concerns = state["fitness_concerns"]
        
        # ç”Ÿæˆåå•†å›å¤
        negotiate_response = await self._llm_generate_negotiate_response(
            fitness_concerns=fitness_concerns,
            collected_info=state["collected_info"],
            conversation_context=self._build_conversation_context(state["messages"])
        )
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append({
            "role": "assistant",
            "content": negotiate_response,
            "type": "fitness_negotiation",
            "fitness_concerns": fitness_concerns
        })
        
        return state
        
    async def _generate_finish_response(self, state: ReasoningState) -> ReasoningState:
        """ç”Ÿæˆæœ€ç»ˆå®Œæˆå›å¤å¹¶å¯åŠ¨æ•…äº‹æ¡†æ¶ç”Ÿæˆ"""
        print("ğŸ‰ ç”Ÿæˆæœ€ç»ˆå®Œæˆå›å¤...")
        
        # ç”Ÿæˆæœ€ç»ˆç¡®è®¤å›å¤
        final_response = await self._llm_generate_final_response(
            collected_info=state["collected_info"],
            sufficiency_scores=state["sufficiency_score"],
            conversation_context=self._build_conversation_context(state["messages"])
        )
        
        # ç”Ÿæˆéœ€æ±‚åˆ†ææŠ¥å‘Š
        analysis_report = await self._generate_requirement_analysis_report(
            state["collected_info"], 
            state["sufficiency_score"]
        )
        
        # æ›´æ–°çŠ¶æ€
        state["messages"].append({
            "role": "assistant",
            "content": final_response,
            "type": "completion_confirmation",
            "analysis_report": analysis_report  # åœ¨æ¶ˆæ¯ä¸­ä¹ŸåŒ…å«æŠ¥å‘Š
        })
        
        # æ ‡è®°ä¸ºready_for_generationï¼Œä½†è¿˜éœ€è¦ç”Ÿæˆæ•…äº‹æ¡†æ¶
        state["final_requirements"] = state["collected_info"].copy()
        state["requirement_analysis_report"] = analysis_report
        
        return state

    async def _generate_story_framework(self, state: ReasoningState) -> ReasoningState:
        """ç”ŸæˆRPGæ•…äº‹æ¡†æ¶"""
        print("ğŸ“š ç”ŸæˆRPGæ•…äº‹æ¡†æ¶...")
        
        # ç”Ÿæˆæ•…äº‹æ¡†æ¶
        story_framework = await self._llm_generate_story_framework(
            state["collected_info"],
            state["sufficiency_score"]
        )
        
        # æ›´æ–°çŠ¶æ€
        state["story_framework"] = story_framework
        state["story_iteration_count"] = state.get("story_iteration_count", 0) + 1
        
        print(f"âœ… æ•…äº‹æ¡†æ¶ç”Ÿæˆå®Œæˆ (ç¬¬{state['story_iteration_count']}æ¬¡)")
        return state

    async def _review_story_framework(self, state: ReasoningState) -> ReasoningState:
        """å®¡æ ¸æ•…äº‹æ¡†æ¶"""
        print("ğŸ” å®¡æ ¸æ•…äº‹æ¡†æ¶è´¨é‡...")
        
        # å®¡æ ¸æ•…äº‹æ¡†æ¶
        review_result = await self._llm_review_story_framework(
            state["collected_info"],
            state["story_framework"]
        )
        
        # æ›´æ–°çŠ¶æ€
        state["story_review_result"] = review_result
        state["story_framework_approved"] = review_result.get("æ˜¯å¦é€šè¿‡", False)
        
        # æ‰“å°å®¡æ ¸ç»“æœ
        total_score = review_result.get("æ€»åˆ†", 0)
        print(f"ğŸ“Š æ•…äº‹æ¡†æ¶å®¡æ ¸å®Œæˆ:")
        print(f"  æ€»åˆ†: {total_score}/100")
        
        dimensions = ["ä¸»çº¿æ˜ç¡®æ€§", "å†…å®¹ä¸€è‡´æ€§", "å‰§æƒ…è¿è´¯æ€§", "æ•™è‚²èåˆåº¦", "å¸å¼•åŠ›è¯„ä¼°"]
        for dim in dimensions:
            if dim in review_result and isinstance(review_result[dim], dict):
                score = review_result[dim].get("åˆ†æ•°", 0)
                print(f"  {dim}: {score}/100")
        
        if state["story_framework_approved"]:
            print("âœ… æ•…äº‹æ¡†æ¶å®¡æ ¸é€šè¿‡ï¼")
            # æ ‡è®°ä¸ºready_for_generationï¼Œè¡¨ç¤ºæ•´ä¸ªStage1+æ•…äº‹æ¡†æ¶ç”Ÿæˆå®Œæˆ
            state["ready_for_generation"] = True
            
            # æ·»åŠ æ•…äº‹æ¡†æ¶å®Œæˆæ¶ˆæ¯ï¼ŒåŒ…å«story_frameworkç”¨äºä¸‹è½½
            completion_message = f"ğŸ‰ RPGæ•…äº‹æ¡†æ¶ç”Ÿæˆå®Œæˆï¼\n\nğŸ“Š æœ€ç»ˆè¯„åˆ†: {total_score}/100\nâœ… æ‰€æœ‰ç»´åº¦è¯„åˆ†å‡è¾¾æ ‡ï¼Œæ•…äº‹æ¡†æ¶å·²é€šè¿‡å®¡æ ¸ã€‚\n\nğŸ® æ‚¨ç°åœ¨å¯ä»¥ä¸‹è½½å®Œæ•´çš„æ•…äº‹æ¡†æ¶è®¾è®¡æ–‡æ¡£ã€‚"
            
            state["messages"].append({
                "role": "assistant", 
                "content": completion_message,
                "type": "story_framework_completion",
                "story_framework": state["story_framework"]  # æ·»åŠ æ•…äº‹æ¡†æ¶ç”¨äºä¸‹è½½
            })
        else:
            print("âŒ æ•…äº‹æ¡†æ¶éœ€è¦æ”¹è¿›")
            improvement_areas = review_result.get("é‡ç‚¹æ”¹è¿›æ–¹å‘", [])
            for area in improvement_areas:
                print(f"  - {area}")
        
        return state

    async def _improve_story_framework(self, state: ReasoningState) -> ReasoningState:
        """æ”¹è¿›æ•…äº‹æ¡†æ¶"""
        print("ğŸ”§ æ”¹è¿›æ•…äº‹æ¡†æ¶...")
        
        # æ”¹è¿›æ•…äº‹æ¡†æ¶
        improved_framework = await self._llm_improve_story_framework(
            state["collected_info"],
            state["story_framework"],
            state["story_review_result"]
        )
        
        # æ›´æ–°çŠ¶æ€
        state["story_framework"] = improved_framework
        state["story_iteration_count"] = state.get("story_iteration_count", 0) + 1
        
        print(f"âœ… æ•…äº‹æ¡†æ¶æ”¹è¿›å®Œæˆ (ç¬¬{state['story_iteration_count']}æ¬¡è¿­ä»£)")
        return state

    async def _distribute_to_levels(self, state: ReasoningState) -> ReasoningState:
        """åˆ†å‘åˆ°æ‰€æœ‰å…³å¡ç”Ÿæˆ - ç®€å•çš„è·¯ç”±èŠ‚ç‚¹"""
        print("ğŸš€ æ•…äº‹æ¡†æ¶å®¡æ ¸é€šè¿‡ï¼Œå¼€å§‹å¹¶è¡Œç”Ÿæˆ6ä¸ªå…³å¡...")
        
        # åˆå§‹åŒ–æ‰€æœ‰å…³å¡çš„çŠ¶æ€å­—æ®µ
        for level in range(1, 7):
            level_key = f"level_{level}"
            if level_key not in state.get("level_details", {}):
                if "level_details" not in state:
                    state["level_details"] = {}
                state["level_details"][level_key] = {
                    "scenes_status": "pending",
                    "characters_status": "pending"
                }
        
        return state
    
    # ==================== LLMè¾…åŠ©æ–¹æ³• ====================
    
    def _build_conversation_context(self, messages: List[Dict[str, str]]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if not messages:
            return "æš‚æ— å¯¹è¯è®°å½•"
            
        context_parts = []
        # å–æœ€è¿‘10è½®å¯¹è¯
        recent_messages = messages[-10:] if len(messages) > 10 else messages
        
        for msg in recent_messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")  # ç§»é™¤é•¿åº¦é™åˆ¶
            context_parts.append(f"{role}: {content}")
            
        return "\n".join(context_parts)
    
    async def _llm_check_input_fitness(self, user_input: str, collected_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ£€æŸ¥ç”¨æˆ·è¾“å…¥çš„é€‚å®œæ€§"""
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_input_fitness_check_prompt()
        fitness_prompt = prompt_template.format(
            user_input=user_input,
            collected_info=self._format_collected_info_for_assessment(collected_info)
        )

        try:
            response = await self.llm.apredict(fitness_prompt)
            json_content = self._extract_json_from_markdown(response.strip())
            result = json.loads(json_content)
            return result
        except Exception as e:
            print(f"âŒ è¾“å…¥é€‚å®œæ€§æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æ‹’ç»çš„ç»“æœ
            return {
                "input_fitness": "rejected",
                "fitness_score": 0,
                "issues": [{
                    "category": "ç³»ç»Ÿé”™è¯¯",
                    "severity": "high",
                    "description": "é€‚å®œæ€§æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                    "suggestion": "è¯·é‡æ–°è¾“å…¥æˆ–è”ç³»ç®¡ç†å‘˜"
                }],
                "assessment_summary": "ç³»ç»Ÿæ£€æŸ¥å¤±è´¥ï¼Œä¸ºå®‰å…¨èµ·è§æ‹’ç»è¾“å…¥"
            }

    async def _llm_assess_sufficiency(self, collected_info: Dict[str, Any], 
                                    conversation_context: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMè¯„ä¼°ä¿¡æ¯è¯¦ç»†åº¦å……è¶³æ€§"""
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_sufficiency_assessment_prompt()
        assessment_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            conversation_context=conversation_context
        )

        try:
            response = await self.llm.apredict(assessment_prompt)
            # æå–markdownä»£ç å—ä¸­çš„JSONå†…å®¹
            json_content = self._extract_json_from_markdown(response.strip())
            result = json.loads(json_content)
            return result
        except Exception as e:
            print(f"âŒ LLMè¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ä½åˆ†è¯„ä¼°
            return {
                "dimension_scores": {
                    "åŸºç¡€ä¿¡æ¯å……è¶³æ€§": 60,
                    "æ•™å­¦ä¿¡æ¯å……è¶³æ€§": 60,
                    "æ¸¸æˆè®¾å®šå……è¶³æ€§": 60,
                    "æƒ…èŠ‚è®¾å®šå……è¶³æ€§": 60
                },
                "overall_score": 60,
                "detailed_feedback": {
                    "strengths": [],
                    "weaknesses": ["è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"],
                    "suggestions": ["è¯·é‡æ–°è¯„ä¼°ä¿¡æ¯è¯¦ç»†åº¦"]
                }
            }
    
    async def _llm_generate_sufficiency_questions(self, collected_info: Dict[str, Any], 
                                                sufficiency_scores: Dict[str, float], 
                                                overall_score: float,
                                                conversation_context: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¯¦ç»†åº¦è¡¥å……é—®é¢˜"""
        
        # æ‰¾åˆ°å¾—åˆ†æœ€ä½çš„ç»´åº¦
        lowest_dimension = min(sufficiency_scores.keys(), key=lambda k: sufficiency_scores[k])
        lowest_score = sufficiency_scores[lowest_dimension]
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_sufficiency_questions_prompt()
        questions_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            sufficiency_scores=sufficiency_scores,
            overall_score=overall_score,
            lowest_dimension=lowest_dimension,
            lowest_score=lowest_score,
            conversation_context=conversation_context
        )

        try:
            return await self.llm.apredict(questions_prompt)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¡¥å……é—®é¢˜å¤±è´¥: {e}")
            return f"ä¸ºäº†æ›´å¥½åœ°è®¾è®¡æ¸¸æˆï¼Œè¯·æä¾›æ›´å¤šå…³äº{lowest_dimension}çš„è¯¦ç»†ä¿¡æ¯ã€‚æ¯”å¦‚æ‚¨å¸Œæœ›æ¸¸æˆå…·ä½“å¦‚ä½•å¸®åŠ©å­¦ç”Ÿå­¦ä¹ ï¼Ÿ"
    
    async def _llm_check_fitness(self, collected_info: Dict[str, Any], 
                               conversation_context: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ£€æŸ¥å†…å®¹é€‚å®œæ€§"""
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_fitness_check_prompt()
        fitness_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            conversation_context=conversation_context
        )

        try:
            response = await self.llm.apredict(fitness_prompt)
            print(f"DEBUG: é€‚å®œæ€§æ£€æŸ¥åŸå§‹å“åº”: {response[:200]}...")
            json_content = self._extract_json_from_markdown(response.strip())
            print(f"DEBUG: æå–çš„JSONå†…å®¹: {json_content[:200]}...")
            result = json.loads(json_content)
            return result
        except Exception as e:
            print(f"âŒ é€‚å®œæ€§æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤é€šè¿‡çš„ç»“æœ
            return {
                "overall_fitness": "é€‚å®œ",
                "concerns": [],
                "positive_aspects": ["å†…å®¹ç§¯æå¥åº·"]
            }
    
    async def _llm_generate_negotiate_response(self, fitness_concerns: List[Dict], 
                                             collected_info: Dict[str, Any],
                                             conversation_context: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆé€‚å®œæ€§åå•†å›å¤"""

        concerns_text = "\n".join([
            f"- {concern.get('category', concern.get('type', 'æœªçŸ¥ç±»åˆ«'))}: {concern.get('description', 'æœªçŸ¥æè¿°')} (ä¸¥é‡ç¨‹åº¦: {concern.get('severity', 'æœªçŸ¥')})"
            for concern in fitness_concerns
        ])
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_negotiate_response_prompt()
        negotiate_prompt = prompt_template.format(
            concerns_text=concerns_text,
            collected_info=self._format_collected_info_for_assessment(collected_info),
            conversation_context=conversation_context
        )

        try:
            return await self.llm.apredict(negotiate_prompt)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆåå•†å›å¤å¤±è´¥: {e}")
            return "å‘ç°ä¸€äº›éœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼Œè¯·ä¿®æ”¹è®¾è®¡ä»¥ç¡®ä¿å†…å®¹æ›´é€‚åˆç›®æ ‡å­¦ç”Ÿç¾¤ä½“ã€‚"
    
    async def _llm_generate_final_response(self, collected_info: Dict[str, Any],
                                         sufficiency_scores: Dict[str, float],
                                         conversation_context: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆç¡®è®¤å›å¤"""
        
        # è®¡ç®—å¹³å‡åˆ†
        average_score = sum(sufficiency_scores.values()) / len(sufficiency_scores) if sufficiency_scores else 75
        
        # ä½¿ç”¨PromptTemplate
        prompt_template = self.prompts.get_finish_response_prompt()
        final_prompt = prompt_template.format(
            collected_info=self._format_collected_info_for_assessment(collected_info),
            sufficiency_scores=sufficiency_scores,
            average_score=average_score,
            conversation_context=conversation_context
        )

        try:
            return await self.llm.apredict(final_prompt)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {e}")
            return "ğŸ‰ ä¿¡æ¯æ”¶é›†å®Œæˆï¼æ‚¨çš„æ•™è‚²æ¸¸æˆè®¾è®¡éå¸¸æ£’ï¼Œæˆ‘ä»¬ç°åœ¨å¼€å§‹ç”Ÿæˆå…·ä½“çš„æ¸¸æˆå†…å®¹ã€‚"
    
    def _extract_json_from_markdown(self, content: str) -> str:
        """ä»markdownä»£ç å—ä¸­æå–JSONå†…å®¹"""
        content = content.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«markdownä»£ç å—
        if content.startswith("```json") and content.endswith("```"):
            # æå–ä»£ç å—ä¸­çš„å†…å®¹
            lines = content.split('\n')
            # ç§»é™¤ç¬¬ä¸€è¡Œçš„```jsonå’Œæœ€åä¸€è¡Œçš„```
            json_lines = lines[1:-1]
            return '\n'.join(json_lines)
        elif content.startswith("```") and content.endswith("```"):
            # å¤„ç†å…¶ä»–ç±»å‹çš„ä»£ç å—
            lines = content.split('\n')
            json_lines = lines[1:-1]
            return '\n'.join(json_lines)
        else:
            # å¦‚æœæ²¡æœ‰ä»£ç å—åŒ…è£…ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
            return content

    def _format_collected_info_for_assessment(self, collected_info: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ”¶é›†çš„ä¿¡æ¯ç”¨äºè¯„ä¼°"""
        formatted_parts = []
        
        # åŸºç¡€ä¿¡æ¯
        if any([collected_info.get("subject"), collected_info.get("grade"), collected_info.get("knowledge_points")]):
            formatted_parts.append("ã€åŸºç¡€ä¿¡æ¯ã€‘")
            if collected_info.get("subject"):
                formatted_parts.append(f"  å­¦ç§‘: {collected_info['subject']}")
            if collected_info.get("grade"):
                formatted_parts.append(f"  å¹´çº§: {collected_info['grade']}")
            if collected_info.get("knowledge_points"):
                points = collected_info['knowledge_points']
                if isinstance(points, list):
                    points = "ã€".join(points)
                formatted_parts.append(f"  çŸ¥è¯†ç‚¹: {points}")

        # æ•™å­¦ä¿¡æ¯
        if any([collected_info.get("teaching_goals"), collected_info.get("teaching_difficulties")]):
            formatted_parts.append("\nã€æ•™å­¦ä¿¡æ¯ã€‘")
            if collected_info.get("teaching_goals"):
                goals = collected_info['teaching_goals']
                if isinstance(goals, list):
                    goals = "ã€".join(goals)
                formatted_parts.append(f"  æ•™å­¦ç›®æ ‡: {goals}")
            if collected_info.get("teaching_difficulties"):
                difficulties = collected_info['teaching_difficulties']
                if isinstance(difficulties, list):
                    difficulties = "ã€".join(difficulties)
                formatted_parts.append(f"  æ•™å­¦éš¾ç‚¹: {difficulties}")

        # æ¸¸æˆè®¾å®š
        if any([collected_info.get("game_style"), collected_info.get("character_design"),
                collected_info.get("world_setting")]):
            formatted_parts.append("\nã€æ¸¸æˆè®¾å®šã€‘")
            if collected_info.get("game_style"):
                formatted_parts.append(f"  æ¸¸æˆé£æ ¼: {collected_info['game_style']}")
            if collected_info.get("character_design"):
                formatted_parts.append(f"  è§’è‰²è®¾è®¡: {collected_info['character_design']}")
            if collected_info.get("world_setting"):
                formatted_parts.append(f"  ä¸–ç•ŒèƒŒæ™¯: {collected_info['world_setting']}")

        # æƒ…èŠ‚è®¾å®š
        if any([collected_info.get("plot_requirements"), collected_info.get("interaction_requirements")]):
            formatted_parts.append("\nã€æƒ…èŠ‚è®¾å®šã€‘")
            if collected_info.get("plot_requirements"):
                plots = collected_info['plot_requirements']
                if isinstance(plots, list):
                    plots = "ã€".join(plots)
                formatted_parts.append(f"  æƒ…èŠ‚éœ€æ±‚: {plots}")
            if collected_info.get("interaction_requirements"):
                interactions = collected_info['interaction_requirements']
                if isinstance(interactions, list):
                    interactions = "ã€".join(interactions)
                formatted_parts.append(f"  äº’åŠ¨æ–¹å¼: {interactions}")

        return "\n".join(formatted_parts) if formatted_parts else "æš‚æ— è¯¦ç»†ä¿¡æ¯"
    # ==================== å…¬å…±æ¥å£æ–¹æ³• ====================
    
    def initialize_reasoning_state(self, session_id: str, user_id: str, 
                                 collected_info: Dict[str, Any]) -> ReasoningState:
        """åˆå§‹åŒ–æ¨ç†çŠ¶æ€"""
        
        return ReasoningState(
            messages=[],
            user_id=user_id,
            
            # éœ€æ±‚æ”¶é›†çŠ¶æ€
            collected_info=collected_info,
            stage1_complete=False,
            
            # === Stage1çŠ¶æ€å­—æ®µ ===
            extracted_info={},
            current_stage="basic_info",
            
            # è¯¦ç»†åº¦è¯„ä¼°çŠ¶æ€  
            sufficiency_score={},
            overall_sufficiency=0.0,
            sufficiency_threshold=75.0,  # å¯é…ç½®
            sufficiency_passed=False,
            
            # è¾“å…¥é€‚å®œæ€§æ£€æŸ¥çŠ¶æ€
            input_fitness_result={},
            input_fitness_passed=True,  # é»˜è®¤é€šè¿‡ï¼Œåªæœ‰æ£€æŸ¥å¤±è´¥æ‰æ ‡è®°ä¸ºFalse
            input_fitness_score=100,
            
            # é€‚å®œæ€§æ£€æŸ¥çŠ¶æ€
            fitness_assessment={},
            fitness_concerns=[],
            fitness_passed=False,
            
            # æ•…äº‹æ¡†æ¶çŠ¶æ€
            story_framework="",
            story_review_result={},
            story_iteration_count=0,
            story_framework_approved=False,
            
            # æœ€ç»ˆçŠ¶æ€
            ready_for_generation=False,
            final_requirements={}
        )
    
    async def process_reasoning_request(self, session_id: str, user_id: str, 
                                      collected_info: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¨ç†è¯·æ±‚çš„ä¸»å…¥å£"""
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = self.initialize_reasoning_state(session_id, user_id, collected_info)
        
        # è¿è¡Œå›¾ - ä½¿ç”¨å›ºå®šthread_idé¿å…å¹¶å‘å†²çª
        thread_config = {"configurable": {"thread_id": "main_thread"}}
        
        try:
            final_state = await self.graph.ainvoke(initial_state, config=thread_config)
            
            return {
                "success": True,
                "final_state": final_state,
                "ready_for_generation": final_state.get("ready_for_generation", False),
                "messages": final_state.get("messages", []),
                "stage": self._determine_current_stage(final_state)
            }
            
        except Exception as e:
            print(f"âŒ StateGraphæ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "ready_for_generation": False
            }
    
    async def process_reasoning_request_with_state(self, reasoning_state: Dict[str, Any], 
                                                  user_input: str) -> Dict[str, Any]:
        """ä½¿ç”¨å·²æœ‰çŠ¶æ€å¤„ç†æ¨ç†è¯·æ±‚ - æ”¯æŒçŠ¶æ€æŒä¹…åŒ–"""
        
        try:
            # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°æ¶ˆæ¯å†å²
            if "messages" not in reasoning_state:
                reasoning_state["messages"] = []
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            reasoning_state["messages"].append({
                "role": "user",
                "content": user_input,
                "timestamp": datetime.now().isoformat()
            })
            
            # åŒæ­¥æ›´æ–°collected_infoçŠ¶æ€
            print(f"DEBUG: åŒæ­¥æ›´æ–°çŠ¶æ€ï¼Œcollected_info: {reasoning_state.get('collected_info', {})}")
            self.collected_info = reasoning_state.get("collected_info", {})
            
            # æ¸…ç©ºä¹‹å‰çš„memoryå¹¶é‡æ–°åŒæ­¥å¯¹è¯å†å²ï¼ˆé¿å…é‡å¤ï¼‰
            self.memory.chat_memory.clear()
            
            # é‡æ–°æ·»åŠ æ‰€æœ‰å†å²æ¶ˆæ¯åˆ°memory
            messages = reasoning_state.get("messages", [])
            for msg in messages:
                if msg.get("role") == "user":
                    self.memory.chat_memory.add_user_message(msg["content"])
                elif msg.get("role") == "assistant":
                    self.memory.chat_memory.add_ai_message(msg["content"])
            
            # è¿è¡Œå›¾ - ä½¿ç”¨å›ºå®šthread_idé¿å…å¹¶å‘å†²çª
            thread_config = {"configurable": {"thread_id": "main_thread"}}
            
            final_state = await self.graph.ainvoke(reasoning_state, config=thread_config)
            
            return {
                "success": True,
                "final_state": final_state,
                "ready_for_generation": final_state.get("ready_for_generation", False),
                "messages": final_state.get("messages", []),
                "stage": self._determine_current_stage(final_state)
            }
            
        except Exception as e:
            print(f"âŒ StateGraphæŒä¹…åŒ–æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "ready_for_generation": False
            }
    
    def _determine_current_stage(self, final_state: ReasoningState) -> str:
        """æ ¹æ®æœ€ç»ˆçŠ¶æ€ç¡®å®šå½“å‰æ‰€å¤„é˜¶æ®µ"""
        
        if final_state.get("ready_for_generation"):
            return "ready_for_generation"
        elif not final_state.get("stage1_complete", False):
            return "stage1_collecting"
        elif not final_state.get("sufficiency_passed", False):
            return "need_more_details"
        elif not final_state.get("fitness_passed", False):
            return "fitness_check"
        else:
            return "unknown"


    # ==================== å…³å¡è¯¦ç»†å†…å®¹ç”ŸæˆèŠ‚ç‚¹ ====================
    
    async def _generate_level_characters(self, state: ReasoningState, level: int) -> ReasoningState:
        """ä¸ºæŒ‡å®šå…³å¡ç”Ÿæˆè§’è‰²å¯¹è¯å’Œè§’è‰²ä»‹ç»"""
        
        try:
            print(f"ğŸ­ å¼€å§‹ç”Ÿæˆç¬¬{level}å…³å¡çš„è§’è‰²å¯¹è¯...")
            
            # åˆå§‹åŒ–level_detailså¦‚æœä¸å­˜åœ¨
            if "level_details" not in state:
                state["level_details"] = {}
            if f"level_{level}" not in state["level_details"]:
                state["level_details"][f"level_{level}"] = {}
            
            # è·å–è§’è‰²å¯¹è¯ç”Ÿæˆprompt
            from prompt_templates import create_prompt_templates
            templates = create_prompt_templates()
            character_prompt = templates.get_level_characters_generation_prompt()
            
            # å‡†å¤‡promptå‚æ•°
            story_framework = state.get("story_framework", "")
            
            # è·å–è¯¥å…³å¡çš„åœºæ™¯æ•°æ®
            level_scenes = ""
            if "level_details" in state and f"level_{level}" in state["level_details"]:
                level_data = state["level_details"][f"level_{level}"]
                if "scenes_script" in level_data:
                    level_scenes = level_data["scenes_script"]
                    print(f"ğŸ¬ è·å–åˆ°ç¬¬{level}å…³å¡çš„åœºæ™¯æ•°æ®ï¼Œé•¿åº¦: {len(level_scenes)}")
                else:
                    print(f"âš ï¸ ç¬¬{level}å…³å¡åœºæ™¯æ•°æ®ä¸å­˜åœ¨")
            else:
                print(f"âš ï¸ ç¬¬{level}å…³å¡level_detailsä¸å­˜åœ¨")
            
            formatted_prompt = character_prompt.format(
                story_framework=story_framework,
                scene_data=level_scenes,  # ä¼ é€’åœºæ™¯æ•°æ®
                level=level
            )
            
            # è°ƒç”¨LLMç”Ÿæˆè§’è‰²å¯¹è¯
            response = await self.llm.ainvoke([{"role": "user", "content": formatted_prompt}])
            characters_content = response.content
            
            # ä¿å­˜ç”Ÿæˆç»“æœ
            state["level_details"][f"level_{level}"]["characters_dialogue"] = characters_content
            state["level_details"][f"level_{level}"]["characters_status"] = "completed"
            state["level_details"][f"level_{level}"]["characters_generated_at"] = datetime.now().isoformat()
            
            print(f"âœ… ç¬¬{level}å…³å¡è§’è‰²å¯¹è¯ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬{level}å…³å¡è§’è‰²å¯¹è¯ç”Ÿæˆå¤±è´¥: {e}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            if "level_details" not in state:
                state["level_details"] = {}
            if f"level_{level}" not in state["level_details"]:
                state["level_details"][f"level_{level}"] = {}
                
            state["level_details"][f"level_{level}"]["characters_status"] = "failed"
            state["level_details"][f"level_{level}"]["characters_error"] = str(e)
        
        return state
    
    async def _generate_level_scenes(self, state: ReasoningState, level: int) -> ReasoningState:
        """ä¸ºæŒ‡å®šå…³å¡ç”Ÿæˆåœºæ™¯è§†è§‰å’Œå‰§æœ¬"""
        
        try:
            print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆç¬¬{level}å…³å¡çš„åœºæ™¯å‰§æœ¬...")
            
            # åˆå§‹åŒ–level_detailså¦‚æœä¸å­˜åœ¨
            if "level_details" not in state:
                state["level_details"] = {}
            if f"level_{level}" not in state["level_details"]:
                state["level_details"][f"level_{level}"] = {}
            
            # è·å–åœºæ™¯å‰§æœ¬ç”Ÿæˆprompt
            from prompt_templates import create_prompt_templates
            templates = create_prompt_templates()
            scene_prompt = templates.get_level_scenes_generation_prompt()
            
            # å‡†å¤‡promptå‚æ•°
            story_framework = state.get("story_framework", "")
            
            formatted_prompt = scene_prompt.format(
                story_framework=story_framework,
                level=level
            )
            
            # è°ƒç”¨LLMç”Ÿæˆåœºæ™¯å‰§æœ¬
            response = await self.llm.ainvoke([{"role": "user", "content": formatted_prompt}])
            scenes_content = response.content
            
            # ä¿å­˜ç”Ÿæˆç»“æœ
            state["level_details"][f"level_{level}"]["scenes_script"] = scenes_content
            state["level_details"][f"level_{level}"]["scenes_status"] = "completed"
            state["level_details"][f"level_{level}"]["scenes_generated_at"] = datetime.now().isoformat()
            
            print(f"âœ… ç¬¬{level}å…³å¡åœºæ™¯å‰§æœ¬ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬{level}å…³å¡åœºæ™¯å‰§æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            if "level_details" not in state:
                state["level_details"] = {}
            if f"level_{level}" not in state["level_details"]:
                state["level_details"][f"level_{level}"] = {}
                
            state["level_details"][f"level_{level}"]["scenes_status"] = "failed"
            state["level_details"][f"level_{level}"]["scenes_error"] = str(e)
        
        return state
    
    async def _collect_all_level_results(self, state: ReasoningState) -> ReasoningState:
        """æ±‡èšæ‰€æœ‰å…³å¡çš„ç”Ÿæˆç»“æœ"""
        
        try:
            print("ğŸ“‹ æ±‡èšæ‰€æœ‰å…³å¡ç”Ÿæˆç»“æœ...")
            
            # ç»Ÿè®¡å®Œæˆæƒ…å†µ
            completed_characters = 0
            completed_scenes = 0
            failed_tasks = []
            
            level_details = state.get("level_details", {})
            
            for level in range(1, 7):
                level_key = f"level_{level}"
                if level_key in level_details:
                    level_data = level_details[level_key]
                    
                    # ç»Ÿè®¡è§’è‰²å¯¹è¯å®Œæˆæƒ…å†µ
                    if level_data.get("characters_status") == "completed":
                        completed_characters += 1
                    elif level_data.get("characters_status") == "failed":
                        failed_tasks.append(f"ç¬¬{level}å…³å¡è§’è‰²å¯¹è¯")
                    
                    # ç»Ÿè®¡åœºæ™¯å‰§æœ¬å®Œæˆæƒ…å†µ
                    if level_data.get("scenes_status") == "completed":
                        completed_scenes += 1
                    elif level_data.get("scenes_status") == "failed":
                        failed_tasks.append(f"ç¬¬{level}å…³å¡åœºæ™¯å‰§æœ¬")
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            summary_lines = [
                "ğŸ‰ å…³å¡è¯¦ç»†å†…å®¹ç”Ÿæˆå®Œæˆï¼",
                f"âœ… è§’è‰²å¯¹è¯ï¼š{completed_characters}/6 ä¸ªå…³å¡å®Œæˆ",
                f"âœ… åœºæ™¯å‰§æœ¬ï¼š{completed_scenes}/6 ä¸ªå…³å¡å®Œæˆ"
            ]
            
            if failed_tasks:
                summary_lines.append(f"âŒ å¤±è´¥ä»»åŠ¡ï¼š{', '.join(failed_tasks)}")
            
            summary_message = "\n".join(summary_lines)
            
            # æ·»åŠ æ±‡æ€»æ¶ˆæ¯
            state["messages"].append({
                "role": "assistant",
                "content": summary_message,
                "type": "level_generation_summary"
            })
            
            # æ›´æ–°çŠ¶æ€
            state["level_generation_status"] = "completed"
            
            print("âœ… å…³å¡ç”Ÿæˆç»“æœæ±‡èšå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ±‡èšç»“æœå¤±è´¥: {e}")
            state["level_generation_status"] = "failed"
            state["messages"].append({
                "role": "assistant",
                "content": f"âŒ å…³å¡å†…å®¹ç”Ÿæˆæ±‡èšå¤±è´¥ï¼š{str(e)}",
                "type": "error"
            })
        
        return state


# ä¾¿åˆ©å‡½æ•°
def create_reasoning_graph() -> ReasoningGraph:
    """åˆ›å»ºReasoningGraphå®ä¾‹çš„ä¾¿åˆ©å‡½æ•°"""
    return ReasoningGraph()